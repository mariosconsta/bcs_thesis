{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxthXpAUDGDe"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau,\n",
        "    TensorBoard,\n",
        ")\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltKoyn25DgFn",
        "outputId": "14c03a86-8f32-436a-c7dd-f45f37b33f9e"
      },
      "outputs": [],
      "source": [
        "#Dataset Path\n",
        "dataset_path = r'.\\dataset'\n",
        "train_path = os.path.join(dataset_path, 'train_set')\n",
        "dev_path = os.path.join(dataset_path, 'dev_set')\n",
        "test_path = os.path.join(dataset_path, 'test_set')\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 16\n",
        "IMG_SIZE = (224, 224) # Resize all images to 224x224 as required by ResNet50\n",
        "\n",
        "#Dataset split\n",
        "train_set = image_dataset_from_directory(\n",
        "    train_path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode = 'categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "dev_set = image_dataset_from_directory(\n",
        "    dev_path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode = 'categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "test_set = image_dataset_from_directory(\n",
        "    test_path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode = 'categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "Oy4au804DqFU",
        "outputId": "f917d835-859c-4897-e20a-771031b9affc"
      },
      "outputs": [],
      "source": [
        "# Visualize Data\n",
        "class_names = train_set.class_names\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_set.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[np.argmax(labels[i])])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFstIW7VI0Ou"
      },
      "outputs": [],
      "source": [
        "# Define Data Augmentation function\n",
        "def data_augmenter():\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(tfl.RandomFlip('horizontal'))\n",
        "    data_augmentation.add(tfl.RandomRotation(0.10))\n",
        "    return data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "cpRvLJoBI07Z",
        "outputId": "7b096145-42de-4ee5-a910-e7441122e3dd"
      },
      "outputs": [],
      "source": [
        "# Visualize Data Augmentation\n",
        "data_augmentation = data_augmenter()\n",
        "\n",
        "for image, _ in train_set.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = image[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0), training = True)\n",
        "        plt.imshow(augmented_image[0] / 255)\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgrubDgQI3bY"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "#   - Define the ResNet50 base model, do not include top (prediction) layers, grab weights from imagenet dataset\n",
        "#   - Freeze layers by setting base_model.trainable = False\n",
        "#   - Define inputs\n",
        "#   - Apply Data Augmentation\n",
        "#   - Preproccess Inputs with ResNet50 preproccess utility\n",
        "#   - Set Training = False for base_model in order not to update Batch Norm values\n",
        "############################################\n",
        "\n",
        "input_shape = IMG_SIZE + (3,)\n",
        "\n",
        "# Define ResNet50 base model with weights from imagenet. Do NOT include top classification layers.\n",
        "base_model = tf.keras.applications.resnet_v2.ResNet50V2(\n",
        "    input_shape=input_shape, include_top=False, weights=\"imagenet\"\n",
        ")\n",
        "\n",
        "# Freeze base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define input layer\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "# Apply Data Augmentation\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "# Preproccess input using the same weights base model was trained on\n",
        "x = tf.keras.applications.resnet_v2.preprocess_input(x)\n",
        "\n",
        "# Set training = False to disable Batch Norm layers from updating\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "# Add avaragePooling\n",
        "x = tfl.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add dropout layer for regularization\n",
        "x = tfl.Dropout(0.2)(x)\n",
        "\n",
        "# Add prediction/output layer with 3 neurons (Class Number = 3)\n",
        "outputs = tfl.Dense(3, activation='softmax', kernel_initializer=HeNormal())(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print Model Summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define metrics\n",
        "metrics = [\n",
        "    tf.keras.metrics.TruePositives(name=\"tp\"),\n",
        "    tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
        "    tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "    tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "    tf.keras.metrics.CategoricalAccuracy(name=\"Accuracy\"),\n",
        "    tf.keras.metrics.Precision(name=\"precision\"),\n",
        "    tf.keras.metrics.Recall(name=\"recall\"),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFOgQ1jbI5R6"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate = 0.004), loss=CategoricalCrossentropy(), metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checkpoint Directory setup\n",
        "def create_checkpoint_dir():\n",
        "    root_dir = os.path.join(os.curdir,\"checkpoints\",\"ResNet50\")\n",
        "    run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')\n",
        "    checkpoint_logs = os.path.join(root_dir, run_id, \"ckp_{epoch:02d}-{val_loss:.4f}\")\n",
        "    return checkpoint_logs\n",
        "\n",
        "checkpoint_logs = create_checkpoint_dir() # .\\checkpoints\\ResNet50\\run_22_07_2022-14_38_09\\ckp_{epoch:02d}-{val_loss:.4f}\n",
        "print(checkpoint_logs)\n",
        "\n",
        "#Tensorboard log directory setup\n",
        "def create_board_dir():\n",
        "    root_dir = os.path.join(os.curdir,\"board_logs\",\"ResNet50\")\n",
        "    run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')\n",
        "    logs = os.path.join(root_dir, run_id)\n",
        "    return logs\n",
        "\n",
        "logs = create_board_dir() # e.g., .\\callbacks\\board_logs\\ResNet50\\run_14-07-2022-15:14:09\n",
        "print(logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRcNQRxlI8Yv"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "checkpoint_cb = ModelCheckpoint(checkpoint_logs, save_weights_only=True, save_best_only=True, monitor='val_loss', verbose=1),\n",
        "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
        "reduce_LR_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),\n",
        "tensorboard_cb = TensorBoard(log_dir=logs,histogram_freq=1, write_images=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i2xd3u-NxU-"
      },
      "outputs": [],
      "source": [
        "callbacks = [checkpoint_cb,early_stopping_cb,reduce_LR_cb,tensorboard_cb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "tVy5vKIyI9vI",
        "outputId": "2a7f7946-c3d5-4700-aa28-e1d183a15dc2"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_set, validation_data = dev_set, epochs=100,verbose=1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model.trainable = True # unfreeze all model layers\n",
        "base_model.summary() # Find a suitable layer name to fine-tune from\n",
        "print(\"Number of layers in base model: \", len(base_model.layers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find layer number you want to fine-tune from\n",
        "index = None\n",
        "for idx, layer in enumerate(base_model.layers):\n",
        "    if layer.name == 'conv4_block6_out': # Put layer name here\n",
        "        index = idx\n",
        "        break\n",
        "print(idx)\n",
        "\n",
        "# Make sure it's the correct index\n",
        "print(base_model.layers[153])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Freeze all until 154\n",
        "fine_tune_from = 154\n",
        "for layer in base_model.layers[:fine_tune_from]:\n",
        "    print('Layer ' + layer.name + ' frozen.')\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compile the model again in order to apply trainable layers change, with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate = 0.00004), loss=CategoricalCrossentropy(), metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finetune model for 10 epochs\n",
        "tuned_epochs = 10\n",
        "total_epochs = len(history.epoch) + tuned_epochs\n",
        "history_tuned = model.fit(train_set, validation_data = dev_set, initial_epoch=history.epoch[-1], epochs=total_epochs,verbose=1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(test_set, batch_size=batch_size, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = np.array([])\n",
        "labels =  np.array([])\n",
        "for x, y in test_set:\n",
        "  predictions = np.concatenate([predictions, np.argmax(model.predict(x), axis = -1)])\n",
        "  labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Classification Report\n",
        "print(classification_report(labels, predictions, target_names=['COVID-19', 'Non-COVID', 'Normal']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcuate Confusion Matrix\n",
        "cm = confusion_matrix(labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot confusion matrix with seaborn\n",
        "plt.figure(figsize=(10, 5))\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cm.flatten()]\n",
        "\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cm.flatten()/np.sum(cm)]\n",
        "\n",
        "cm_labels = [f\"{v1}\\n{v2}\\n\" for v1, v2 in\n",
        "          zip(group_counts,group_percentages)]\n",
        "\n",
        "cm_labels = np.asarray(cm_labels).reshape(3,3)\n",
        "\n",
        "sns.set(font_scale=1.4)\n",
        "ax = sns.heatmap(cm, annot=cm_labels, fmt='', linewidths=.5, cmap='YlGnBu')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.set_xlabel('Predicted Values')\n",
        "ax.set_ylabel('Actual values')\n",
        "\n",
        "ax.xaxis.set_ticklabels(['COVID-19','Non-COVID19', 'Normal'])\n",
        "ax.yaxis.set_ticklabels(['COVID-19','Non-COVID19', 'Normal'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ResNet50.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "5ec7005f4a9b61889b909841db5d74fb8404e085e9bff40e5d844aa85c5d903b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
