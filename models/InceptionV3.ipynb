{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxthXpAUDGDe"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    "    CSVLogger\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltKoyn25DgFn",
    "outputId": "14c03a86-8f32-436a-c7dd-f45f37b33f9e"
   },
   "outputs": [],
   "source": [
    "#Dataset Path\n",
    "dataset_path = r'.\\dataset'\n",
    "train_path = os.path.join(dataset_path, 'train_set')\n",
    "dev_path = os.path.join(dataset_path, 'dev_set')\n",
    "test_path = os.path.join(dataset_path, 'test_set')\n",
    "\n",
    "#Hyperparameters\n",
    "batch_size = 16\n",
    "IMG_SIZE = (256, 256) # Do not resize images, 256x256 is the default size\n",
    "\n",
    "#Dataset split\n",
    "train_set = image_dataset_from_directory(\n",
    "    train_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = 'categorical',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "dev_set = image_dataset_from_directory(\n",
    "    dev_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = 'categorical',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_set = image_dataset_from_directory(\n",
    "    test_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = 'categorical',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_set:\n",
    "  print(\"image shape is:\", image_batch.shape)\n",
    "  print(\"labels shape is:\", labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "Oy4au804DqFU",
    "outputId": "f917d835-859c-4897-e20a-771031b9affc"
   },
   "outputs": [],
   "source": [
    "# Visualize Data\n",
    "class_names = train_set.class_names\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[np.argmax(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFstIW7VI0Ou"
   },
   "outputs": [],
   "source": [
    "# Define Data Augmentation function\n",
    "def data_augmenter():\n",
    "    data_augmentation = tf.keras.Sequential()\n",
    "    data_augmentation.add(tfl.RandomFlip('horizontal'))\n",
    "    data_augmentation.add(tfl.RandomRotation(0.10))\n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "cpRvLJoBI07Z",
    "outputId": "7b096145-42de-4ee5-a910-e7441122e3dd"
   },
   "outputs": [],
   "source": [
    "# Visualize Data Augmentation\n",
    "data_augmentation = data_augmenter()\n",
    "\n",
    "for image, _ in train_set.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = image[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0), training = True)\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgrubDgQI3bY"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "#   - Define the base model, do not include top (prediction) layers, grab weights from imagenet dataset\n",
    "#   - Freeze layers by setting base_model.trainable = False\n",
    "#   - Define inputs\n",
    "#   - Apply Data Augmentation\n",
    "#   - Preproccess Inputs with the preproccess utility.\n",
    "#   - Set Training = False for base_model in order not to update Batch Norm values\n",
    "############################################\n",
    "\n",
    "input_shape = IMG_SIZE + (3,)\n",
    "\n",
    "# Define the base model with weights from imagenet. Do NOT include top classification layers.\n",
    "base_model = tf.keras.applications.inception_v3.InceptionV3(\n",
    "    input_shape=input_shape, include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define input layer\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Apply Data Augmentation\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Preproccess input using the same weights base model was trained on\n",
    "x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "\n",
    "# Set training = False to disable Batch Norm layers from updating\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Add flatten layer. GAP layer was causing issues during fine-tuning\n",
    "x = tfl.Flatten()(x)\n",
    "\n",
    "# Add dropout layer for regularization\n",
    "x = tfl.Dropout(0.2)(x)\n",
    "\n",
    "# Add prediction/output layer with 3 neurons (Class Number = 3)\n",
    "outputs = tfl.Dense(3, activation='softmax', kernel_initializer=HeNormal())(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "metrics = [\n",
    "    tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "    tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    tf.keras.metrics.CategoricalAccuracy(name=\"Accuracy\"),\n",
    "    tf.keras.metrics.Precision(name=\"precision\"),\n",
    "    tf.keras.metrics.Recall(name=\"recall\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFOgQ1jbI5R6"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate = 0.004), loss=CategoricalCrossentropy(), metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint directory setup\n",
    "def create_checkpoint_dir():\n",
    "    root_dir = os.path.join(os.curdir,\"checkpoints\",\"InceptionV3\")\n",
    "    run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')\n",
    "    checkpoint_logs = os.path.join(root_dir, run_id, \"ckp_{epoch:02d}-{val_loss:.4f}\")\n",
    "    return checkpoint_logs\n",
    "\n",
    "checkpoint_logs = create_checkpoint_dir()\n",
    "print(checkpoint_logs)\n",
    "\n",
    "#Tensorboard log directory setup\n",
    "def create_board_dir():\n",
    "    root_dir = os.path.join(os.curdir,\"board_logs\",\"InceptionV3\")\n",
    "    run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')\n",
    "    logs = os.path.join(root_dir, run_id)\n",
    "    return logs\n",
    "\n",
    "logs = create_board_dir()\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRcNQRxlI8Yv"
   },
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(checkpoint_logs, save_weights_only=True, save_best_only=True, monitor='val_loss', verbose=1),\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
    "reduce_LR_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),\n",
    "tensorboard_cb = TensorBoard(log_dir=logs,histogram_freq=1, write_images=True)\n",
    "logger_cb = CSVLogger('logs/InceptionV3/log.csv', append=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i2xd3u-NxU-"
   },
   "outputs": [],
   "source": [
    "callbacks = [checkpoint_cb,early_stopping_cb,reduce_LR_cb,tensorboard_cb, logger_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "tVy5vKIyI9vI",
    "outputId": "2a7f7946-c3d5-4700-aa28-e1d183a15dc2"
   },
   "outputs": [],
   "source": [
    "# Train initial model without fine-tuning\n",
    "history = model.fit(train_set, validation_data = dev_set, epochs=100,verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore weights from best checkpoint if needed\n",
    "# model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save initial training history\n",
    "logged_hist = pd.read_csv('logs/InceptionV3/log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View logged history\n",
    "logged_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True # unfreeze all model layers\n",
    "# Print all layers of base_model\n",
    "for layer in base_model.layers[:-1]:\n",
    "    print('Layer ' + layer.name)\n",
    "print(\"Number of layers in base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find layer number you want to fine-tune from\n",
    "index = None\n",
    "for idx, layer in enumerate(base_model.layers):\n",
    "    if layer.name == 'average_pooling2d_13': # Put layer name here\n",
    "        index = idx\n",
    "        break\n",
    "print(idx)\n",
    "\n",
    "# Make sure it's the correct index\n",
    "print(base_model.layers[151])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all until selected layer\n",
    "fine_tune_from = 152\n",
    "for layer in base_model.layers[:fine_tune_from]:\n",
    "    print('Layer ' + layer.name + ' frozen.')\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model again in order to apply trainable layers change, with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate = 0.0004), loss=CategoricalCrossentropy(), metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify epoch numbers for fine-tuning\n",
    "fine_tune_epochs = 10\n",
    "total_epochs = len(logged_hist.epoch) + fine_tune_epochs\n",
    "print(\"Total Epochs without finetuning:\", len(logged_hist.epoch))\n",
    "print(\"Total Epochs with finetuning:\", total_epochs)\n",
    "print(\"Finetuning will continue from epoch:\", logged_hist.iloc[-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model again for a few epochs\n",
    "history_tuned = model.fit(train_set, validation_data = dev_set, initial_epoch=logged_hist.iloc[-1,0], epochs=total_epochs,verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_set, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([])\n",
    "labels =  np.array([])\n",
    "for x, y in test_set:\n",
    "  predictions = np.concatenate([predictions, np.argmax(model.predict(x), axis = -1)])\n",
    "  labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Classification Report\n",
    "print(classification_report(labels, predictions, target_names=['COVID-19', 'Non-COVID', 'Normal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate Confusion Matrix\n",
    "cm = confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plot confusion matrix with seaborn\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "\n",
    "cm_labels = [f\"{v1}\\n{v2}\\n\" for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "\n",
    "cm_labels = np.asarray(cm_labels).reshape(3,3)\n",
    "\n",
    "sns.set(font_scale=1.4)\n",
    "ax = sns.heatmap(cm, annot=cm_labels, fmt='', linewidths=.5, cmap='YlGnBu')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('Actual values')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['COVID-19','Non-COVID19', 'Normal'])\n",
    "ax.yaxis.set_ticklabels(['COVID-19','Non-COVID19', 'Normal'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet50.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.8 ('bsc_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "697d675a9d18f240564d470ca5cf88a0a0ab6899322889d6ebdc57f2a3a57c15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
